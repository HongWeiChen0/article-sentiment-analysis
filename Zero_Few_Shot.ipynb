{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhbKaI0mWPxH"
      },
      "source": [
        "# **Author Sentitment Prediction With PerSent Dataset** #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR1k0C5IXPoj"
      },
      "source": [
        "The goal of this project is to use large language models to predict the author sentitment on an entity in news articles.\n",
        "\n",
        "README:\n",
        "- The Flan T5 model was called using HuggingFace's API: https://api-inference.huggingface.co/models/google/flan-t5-xxl\n",
        "- To call the API, please change the authorization token in the code below\n",
        "- Zero-Shot Prompt: \"classify the author sentiment on {row['TARGET_ENTITY']} as positive, neutral, or negative:{newline}{document}\"\n",
        "- Few-Shot Prompt: \"classify the author sentiment on {row['TARGET_ENTITY']} as positive, neutral, or negative:{newline}{example1}{newline}{example2}{newline}{example3}{newline}{document}\"\n",
        "- Example 1 : \"- Example: John Smith, a leading economist, has criticized President Biden's handling of the economy, saying that his policies have contributed to a major downturn in the job market. : This is a negative sentiment on President Biden.\"\n",
        "- Example 2 : \"- Example: President Smith has committed to taking strong action against climate change, saying that it is one of the greatest threats facing humanity. : This is a positive sentiment on President Smith.\"\n",
        "- Example 3 : \"- Example: A panel of experts evaluates Dr. Jane Doe's contributions to medical research. : This is a neutral sentiment on Dr. Jane Doe.\"\n",
        "- To run the code below, set the location to your Google Drive folder containing the PerSent dataset found here: https://stonybrooknlp.github.io/PerSenT/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfYQ7zUSWz7R"
      },
      "source": [
        "## **Zero Shot flan-t5-xxl** ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okr9Mz4vJdb5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "from google.colab import drive\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCAyrahMMUZb",
        "outputId": "ae419184-3dce-4520-cd7f-e0c564ba7964"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/NLP Final Project\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive',force_remount=True)\n",
        "%cd \"/content/drive/MyDrive/NLP Final Project\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAtCmxyBMfuq"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('random_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-xcBu8RNP-j"
      },
      "outputs": [],
      "source": [
        "API_URL = \"https://api-inference.huggingface.co/models/google/flan-t5-xxl\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f31rHUgFNb1t"
      },
      "outputs": [],
      "source": [
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn json.loads(response.content.decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_A5v0uy_c7U5"
      },
      "outputs": [],
      "source": [
        "#Manually saving partial predictions results because Huggingface's rate limit.\n",
        "predictions = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "em2p_k1cOimH"
      },
      "outputs": [],
      "source": [
        "newline = '\\n'\n",
        "def run_flan_t5_xxl_prompting(df):\n",
        "    for index, row in df.iterrows():\n",
        "        #start from where huggingface limit cut from last time\n",
        "        if index <= 546:\n",
        "            continue\n",
        "        document = row['DOCUMENT']\n",
        "        if len(document) > 3500:\n",
        "            document = document[0:3500]\n",
        "        input = f\"classify the author sentiment on {row['TARGET_ENTITY']} as positive, neutral, or negative:{newline}{document}\"\n",
        "        parameters = {'inputs': input}\n",
        "        prediction = query(parameters)[0]['generated_text']\n",
        "        predictions[index] = prediction\n",
        "        print(index, len(document), document[0:10], prediction)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVbdpZcrTBVz"
      },
      "outputs": [],
      "source": [
        "df = run_flan_t5_xxl_prompting(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcnH28FYdJvm"
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBzi-rjTfe_c"
      },
      "outputs": [],
      "source": [
        "dic = pd.DataFrame.from_dict(predictions, orient='index', columns=['prediction'])\n",
        "dic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvLx-KrSgVXw"
      },
      "outputs": [],
      "source": [
        "dic.to_csv('temp_result.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "values = list(predictions.values())"
      ],
      "metadata": {
        "id": "RV5VnqzE3a-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_mGI0aQTLtu"
      },
      "outputs": [],
      "source": [
        "df['predictions'] = values\n",
        "df.to_csv('random_test_output.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Few Shot flan-t5-xxl** ##"
      ],
      "metadata": {
        "id": "q7j9mlI36p_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('random_test.csv')"
      ],
      "metadata": {
        "id": "R1jvr0IH6tHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Manually saving partial predictions results because Huggingface's rate limit.\n",
        "few_predictions = {}"
      ],
      "metadata": {
        "id": "Q2K_FV4m68Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newline = '\\n'\n",
        "def few_run_flan_t5_xxl_prompting(df):\n",
        "    for index, row in df.iterrows():\n",
        "        #start from where huggingface limit cut from last time\n",
        "        if index <= 507:\n",
        "            continue\n",
        "        document = row['DOCUMENT']\n",
        "        if len(document) > 3200:\n",
        "            document = document[0:3200]\n",
        "        example1 = \"- Example: John Smith, a leading economist, has criticized President Biden's handling of the economy, saying that his policies have contributed to a major downturn in the job market. : This is a negative sentiment on President Biden.\"\n",
        "        example2 = \"- Example: President Smith has committed to taking strong action against climate change, saying that it is one of the greatest threats facing humanity. : This is a positive sentiment on President Smith.\"\n",
        "        example3 = \"- Example: A panel of experts evaluates Dr. Jane Doe's contributions to medical research. : This is a neutral sentiment on Dr. Jane Doe.\"\n",
        "        input = f\"classify the author sentiment on {row['TARGET_ENTITY']} as positive, neutral, or negative:{newline}{example1}{newline}{example2}{newline}{example3}{newline}{document}\"\n",
        "        parameters = {'inputs': input}\n",
        "        prediction = query(parameters)[0]['generated_text']\n",
        "        few_predictions[index] = prediction\n",
        "        print(index, len(document), document[0:10], prediction)"
      ],
      "metadata": {
        "id": "miFau4976-ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_run_flan_t5_xxl_prompting(df)"
      ],
      "metadata": {
        "id": "oCDPK_-P7-_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_predictions"
      ],
      "metadata": {
        "id": "R6nQU2myCm12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values = list(few_predictions.values())"
      ],
      "metadata": {
        "id": "RAZJoCAGEzJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['predictions'] = values\n",
        "df.to_csv('few_random_test_output.csv', index=False)"
      ],
      "metadata": {
        "id": "old4GuqdE60d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Performance Analysis**##"
      ],
      "metadata": {
        "id": "TGhIzDElvMJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('random_test_output.csv')\n",
        "answers = df['TRUE_SENTIMENT'].tolist()\n",
        "predictions = df['predictions'].tolist()\n",
        "labels = ['Positive', 'Neutral', 'Negative']\n",
        "for index, value in enumerate(predictions):\n",
        "    predictions[index] = predictions[index].capitalize()"
      ],
      "metadata": {
        "id": "IH64ANMFvVWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(answers, predictions, labels=labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rO6buCC32rMO",
        "outputId": "a908ae7c-5bb4-4dc2-d9ea-4b9a043b6bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Positive       0.66      0.69      0.68       293\n",
            "     Neutral       0.44      0.05      0.09       213\n",
            "    Negative       0.25      0.84      0.38        73\n",
            "\n",
            "   micro avg       0.47      0.47      0.47       579\n",
            "   macro avg       0.45      0.53      0.38       579\n",
            "weighted avg       0.53      0.47      0.42       579\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('few_random_test_output.csv')\n",
        "few_answers = df['TRUE_SENTIMENT'].tolist()\n",
        "few_predictions = df['predictions'].tolist()\n",
        "labels = ['Positive', 'Neutral', 'Negative']\n",
        "for index, value in enumerate(few_predictions):\n",
        "    few_predictions[index] = few_predictions[index].capitalize()"
      ],
      "metadata": {
        "id": "G_O9WiFK4RTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(few_answers, few_predictions, labels=labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku6PEwq54qBN",
        "outputId": "829be250-4014-48bd-a511-5fc0acba8be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Positive       0.64      0.68      0.66       293\n",
            "     Neutral       0.48      0.05      0.09       213\n",
            "    Negative       0.24      0.79      0.37        73\n",
            "\n",
            "   micro avg       0.46      0.46      0.46       579\n",
            "   macro avg       0.45      0.51      0.37       579\n",
            "weighted avg       0.53      0.46      0.41       579\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Hypotheses Testing**##"
      ],
      "metadata": {
        "id": "y-ptGtnmUJr0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Long Inputs**###"
      ],
      "metadata": {
        "id": "KNNoCLQfUQUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('random_test_output.csv')\n",
        "articles = df['DOCUMENT'].tolist()\n",
        "true_sentiment = df['TRUE_SENTIMENT'].tolist()\n",
        "predictions = df['predictions'].tolist()\n",
        "for index, value in enumerate(articles):\n",
        "    if len(articles[index]) > 3500:\n",
        "        print(articles[index], true_sentiment[index], predictions[index])"
      ],
      "metadata": {
        "id": "DWlSbveyUJSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['Positive', 'Neutral', 'Negative']\n",
        "short_true_sentiment = []\n",
        "short_predictions = []\n",
        "for index, value in enumerate(articles):\n",
        "    if len(articles[index]) <= 3500:\n",
        "        short_true_sentiment.append(true_sentiment[index])\n",
        "        short_predictions.append(predictions[index].capitalize())\n",
        "print(classification_report(short_true_sentiment, short_predictions, labels=labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OECsikQsXjUp",
        "outputId": "b08abf8c-f62b-494a-bdea-58fa2da47030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Positive       0.65      0.69      0.67       254\n",
            "     Neutral       0.46      0.06      0.11       185\n",
            "    Negative       0.24      0.82      0.38        62\n",
            "\n",
            "    accuracy                           0.47       501\n",
            "   macro avg       0.45      0.52      0.38       501\n",
            "weighted avg       0.53      0.47      0.42       501\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Neutral Sentiment**###"
      ],
      "metadata": {
        "id": "10PkwSFxaW1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_neutral_true_sentiment = []\n",
        "no_neutral_predictions = []\n",
        "for index, value in enumerate(articles):\n",
        "    if true_sentiment[index] != 'Neutral':\n",
        "        no_neutral_true_sentiment.append(true_sentiment[index])\n",
        "        no_neutral_predictions.append(predictions[index].capitalize())\n",
        "print(classification_report(no_neutral_true_sentiment, no_neutral_predictions, labels=labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b0auPB2aaY9",
        "outputId": "45589d2d-1e99-46bc-9d34-0760eec3b32a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Positive       0.95      0.69      0.80       293\n",
            "     Neutral       0.00      0.00      0.00         0\n",
            "    Negative       0.44      0.84      0.58        73\n",
            "\n",
            "   micro avg       0.72      0.72      0.72       366\n",
            "   macro avg       0.46      0.51      0.46       366\n",
            "weighted avg       0.85      0.72      0.75       366\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Mixed Sentiment**###"
      ],
      "metadata": {
        "id": "VZkETiY2QGm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_without_negative_sentiment = \" Meanwhile  the creation of a joint cabinet and punishment of coup leaders remain dependent on Zelaya's return to the presidency  still far from certain four months into the standoff that emerged from the coup. Union leader Juan Barahona  one of Zelaya's top three negotiators  told a rally of hundreds of the president's followers that the joint cabinet  if indeed formed  would be made up of ministers from both governments. The formation of a national unity government and amnesty for crimes linked to the coup were two key points of the San Jose reconciliation agenda set out in August  whose central tenet calls for Zelaya's return to office. The resumption of talks on Tuesday will come just two days before the October 15 deadline given by the Zelaya camp for his unconditional return to power. Reinstating him any later  supporters say  risks causing a delay in presidential and legislative elections planned for November 29. \\\"I do not understand the three-day break \\\" Zelaya's wife Xiomara Castro told AFP from within the Brazilian embassy  where the deposed leader has been holed up since his surprise return to the capital on September 21. A diplomatic delegation from the Organization of American States left Honduras Thursday without resolving the political impasse between Micheletti and Zelaya  who was forced out of the country at gunpoint. A rancher known for his trademark white cowboy hat  Zelaya veered to the left after his election and alarmed conservatives by aligning himself with leftist Venezuelan President Hugo Chavez. They feared Zelaya was seeking to change the constitution to allow himself to seek reelection.\"\n",
        "input = f\"classify the author sentiment on Manuel Zelaya as positive, neutral, or negative:{newline}{document_without_negative_sentiment}\"\n",
        "parameters = {'inputs': input}\n",
        "prediction = query(parameters)[0]['generated_text']\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1mLAdZRQF-f",
        "outputId": "62cfb076-3175-4288-e13d-8583744e8241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}