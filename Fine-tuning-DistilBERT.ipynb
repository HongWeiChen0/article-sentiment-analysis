{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWMGPce-Cu_J"
      },
      "source": [
        "# **CSE 354 Project**\n",
        "---\n",
        "## **Problem statement**\n",
        "\n",
        "In this project, we will be using language models to predict the sentiment of a given news article. The dataset is sampled from the [PerSent](https://stonybrooknlp.github.io/PerSenT/) corpus. The data contains around 5k documents and 38K paragraphs annotated on the author’s sentiment towards the main entity in the news article. The label can either be *positive*, *negative*, or *neutral*. We have been given four files - train_data.csv, val_data.csv, random_test.csv, and fixed_test.csv. The training data will be used to fine-tune the language model, the val data will be used to evaluate the training, and finally the test data will test on randomly organized test instances.\n",
        "\n",
        "To perform this task we will be using a pre-trained DistilBERT model. DistilBERT is a BERT based language model. Its size is 40% lesser than BERT, it has around 97% of BERT's language understanding capabilities and is 60% faster. You can read more about DistilBERT - https://arxiv.org/abs/1910.01108.\n",
        "\n",
        "We will be using the model by taking advantage of the libraries provided by Hugging Face (https://huggingface.co/). In order to use this library, it will need to be installed using the command in the cell below. We will be training four different DistilBERT models for this project.\n",
        "\n",
        "Code based on CSE 354 HW3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "om1HnMs-Gw3c",
        "outputId": "54e3c540-c422-475c-e8f9-6227eada99e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6-oi7JhGjjB"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0N6kcCYG3tn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import AdamW\n",
        "import os\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8rQ5vERGwes"
      },
      "source": [
        "## **Mounting Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIrvZGP08z-1",
        "outputId": "08c91113-a2d5-4d62-fae9-0f624ccbe7ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFyqw_sxnEh8",
        "outputId": "1fb3141d-1526-4b97-b9ee-51d14339faf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVvQyFjTm4hW",
        "outputId": "ae502b4d-5f4a-4c99-8ec9-9d3f74addae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CSE-354-Project\n"
          ]
        }
      ],
      "source": [
        "%cd \"drive/MyDrive/CSE-354-Project\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kihyF-BLHgou"
      },
      "source": [
        "## **Constants**\n",
        "\n",
        "The code block below contains a few constants.\n",
        "\n",
        "\n",
        "1.   **BATCH_SIZE**: The batch size input to the models. This has been set to 16. In case we encounter any CUDA - out of memory errors while training our models, this value may be reduced from 16.\n",
        "2.   **EPOCHS**: The number of epochs to train our model.\n",
        "3. **TEST_PATH**: This is the path to the test_data.csv file.\n",
        "4. **TRAIN_PATH**: This is the path to the train_data.csv file.\n",
        "5. **VAL_PATH**: This is the path to the val_data.csv file.\n",
        "6. **SAVE_PATH**: This is the path to the directory our model will be saved. Note: This path will be altered further down in the code by appending the name of the kind of DistilBERT model we train as per our experiments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrivEEHR0kUq"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "EPOCHS = 3\n",
        "TEST_PATH = \"data/random_test.csv\"\n",
        "TRAIN_PATH = \"data/train.csv\"\n",
        "VAL_PATH = \"data/dev.csv\"\n",
        "SAVE_PATH = \"models/DistilBERT\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iuId22dbbSq"
      },
      "outputs": [],
      "source": [
        "def load_dataset(path):\n",
        "  dataset = pd.read_csv(path)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQk4i6bH-LUo"
      },
      "outputs": [],
      "source": [
        "train_data = load_dataset(TRAIN_PATH)\n",
        "val_data = load_dataset(VAL_PATH)\n",
        "test_data = load_dataset(TEST_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz5glgyTJRkL"
      },
      "source": [
        "## **Initialize the Model Class**\n",
        "\n",
        "Here, we will setup the pre-trained DistillBert model class in order to do our trinary sentiment analysis task. In the code block below, we load a pre-trained DistilBERT model and its tokenizer using Hugging Face's library. The model we load is called \"distilbert-base-uncased\". It has the model hyperparameter set to *num_classes* as the output shape of the model (in this case it is going to be 3, positive, negative, and neutral).\n",
        "\n",
        "\n",
        "\n",
        "More about the model and how to load it can be read at - https://huggingface.co/distilbert-base-uncased."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmXIi5Y8fmcn"
      },
      "outputs": [],
      "source": [
        "class DistillBERT():\n",
        "\n",
        "  def __init__(self, model_name='distilbert-base-uncased', num_classes=3):\n",
        "    self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = num_classes)\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "  def get_tokenizer_and_model(self):\n",
        "    return self.model, self.tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZWzKZbiKMFP"
      },
      "source": [
        "## **Initialize the Dataloader Class**\n",
        "\n",
        "Here, we will setup the dataloader class which will read data, tokenize it using the DistillBert tokenizer, converts the tokenized sentence to tensors and the labels to tensors. The code block below takes our dataset (train, validation, or test) and the tokenizer we loaded in the previous block and generates the DataLoader object for it. We implement a tokenize_data method that takes the given data and generates a list of token IDs for a given article along with its label. We use the tokenizer to generated the token ids using tokenizer.encode_plus values for each article. We ensure that the maximum length of an encoded article is 512 tokens. If any input data is longer than 512 words/tokens, we truncate it to first 512. We also convert the labels to a corresponding numerical class using the label_dict dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yJHPbHud-nH"
      },
      "outputs": [],
      "source": [
        "class DatasetLoader(Dataset):\n",
        "\n",
        "  def __init__(self, data, tokenizer):\n",
        "    self.data = data\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def tokenize_data(self):\n",
        "    print(\"Processing data..\")\n",
        "    tokens = []\n",
        "    labels = []\n",
        "    label_dict = {'Positive': 2, 'Negative': 1, 'Neutral': 0}\n",
        "\n",
        "    review_list = self.data['DOCUMENT'].to_list()\n",
        "    label_list = self.data['TRUE_SENTIMENT'].to_list()\n",
        "\n",
        "    for (review, label) in tqdm(zip(review_list, label_list), total=len(review_list)):\n",
        "      encoding = self.tokenizer.encode_plus(review, truncation=True, max_length=512)\n",
        "      input_ids = encoding['input_ids']\n",
        "      tokens.append(torch.tensor(input_ids))\n",
        "      labels.append((label_dict[label]))\n",
        "\n",
        "    tokens = pad_sequence(tokens, batch_first=True)\n",
        "    labels = torch.tensor(labels)\n",
        "    dataset = TensorDataset(tokens, labels)\n",
        "    return dataset\n",
        "\n",
        "  def get_data_loaders(self, batch_size=32, shuffle=True):\n",
        "    processed_dataset = self.tokenize_data()\n",
        "\n",
        "    data_loader = DataLoader(\n",
        "        processed_dataset,\n",
        "        shuffle=shuffle,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIF3eaP3Lh0W"
      },
      "source": [
        "## **Training Function**\n",
        "\n",
        "Here, we write the code that will be used to run our model class on the dataset class, both of which we have written in the previously.\n",
        "\n",
        "The class below provides methods to train a given model. It takes a dictionary with the following parameters:\n",
        "\n",
        "\n",
        "1.   device: The device to run the model on.\n",
        "2.   train_data: The train_data dataframe.\n",
        "3.   val_data: The val_data dataframe.\n",
        "4.   batch_size: The batch_size which is input to the model.\n",
        "5.   epochs: The number of epochs to train the model.\n",
        "6.   training_type: The type of training that our model will be undergoing. This can take four values - 'frozen_embeddings', 'top_2_training', 'top_4_training' and 'all_training'.\n",
        "\n",
        "#### **Set Training Parameters**\n",
        "\n",
        "Here we implement the set_training_parameters() method. In this method we select the layers of our model to train based on the training_type. **Note: By default the Hugging Face DistilBERT has 6 layers.**\n",
        "\n",
        "1. frozen_embeddings: This setting trains the DistilBERT model with embeddings that are 'frozen' i.e., not trainable. We ensure that the embedding layers in our model are not trainable.\n",
        "2. top_2_training: This setting trains just the final two layers of DistilBERT (layer 5 and layer 4). All other layers before these are frozen.\n",
        "3. top_4_training: This setting trains just the final four layers of DistilBERT (layer 5, layer 4, layer 3 and layer 2). All other layers before these are frozen.\n",
        "4. all_training: All layers of DistilBERT are trained.\n",
        "\n",
        "**Note: The classifier head on top of the final DistilBERT layer is always trained, so we do not freeze that layer.**\n",
        "\n",
        "**Note: We use model.named_parameters() to iterate over all the named parameters of the model. To set the layers to not be trainable, we apply layer.requires_grad = false**\n",
        "\n",
        "\n",
        "#### **Single Training Step**\n",
        "\n",
        "Here we implement a single training step in the given loop inside the train() method. We pass the review and label in the given batch to the model, take the output and compute the Precision, Recall and F1 for that batch using the get_performance_metrics() method. We also propagate the loss backwards to the model and update the given optimizer's parameters.\n",
        "\n",
        "\n",
        "#### **Single Validation Step**\n",
        "\n",
        "Here we implement a single validation step in the given loop inside the eval() method. We pass the review and label in the given batch to the model, take the output and compute the Precision, Recall and F1 for that batch using the get_performance_metrics() method. We ensure that the loss is not propagated backwards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llVpLV9kiUST"
      },
      "outputs": [],
      "source": [
        "class Trainer():\n",
        "\n",
        "  def __init__(self, options):\n",
        "    self.device = options['device']\n",
        "    self.train_data = options['train_data']\n",
        "    self.val_data = options['val_data']\n",
        "    self.batch_size = options['batch_size']\n",
        "    self.epochs = options['epochs']\n",
        "    self.save_path = options['save_path']\n",
        "    self.training_type = options['training_type']\n",
        "    transformer = DistillBERT()\n",
        "    self.model, self.tokenizer = transformer.get_tokenizer_and_model()\n",
        "    self.model.to(self.device)\n",
        "\n",
        "  def get_performance_metrics(self, preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    precision = precision_score(labels_flat, pred_flat, zero_division=0, average='micro')\n",
        "    recall = recall_score(labels_flat, pred_flat, zero_division=0, average='micro')\n",
        "    f1 = f1_score(labels_flat, pred_flat, zero_division=0, average='micro')\n",
        "    return precision, recall, f1\n",
        "\n",
        "  def set_training_parameters(self):\n",
        "    if self.training_type == 'frozen_embeddings':\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if 'embeddings' in name:\n",
        "                param.requires_grad = False\n",
        "    elif self.training_type == 'top_2_training':\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if 'embeddings' in name:\n",
        "                param.requires_grad = False\n",
        "            if 'layer.0' in name:\n",
        "                param.requires_grad = False\n",
        "            if 'layer.1' in name:\n",
        "                param.requires_grad = False\n",
        "            if 'layer.2' in name:\n",
        "                param.requires_grad = False\n",
        "            if 'layer.3' in name:\n",
        "                param.requires_grad = False\n",
        "    elif self.training_type == 'top_4_training':\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if 'embeddings' in name:\n",
        "                param.requires_grad = False\n",
        "            if 'layer.0' in name:\n",
        "                param.requires_grad = False\n",
        "            if 'layer.1' in name:\n",
        "                param.requires_grad = False\n",
        "\n",
        "  def train(self, data_loader, optimizer):\n",
        "    self.model.train()\n",
        "    total_recall = 0\n",
        "    total_precision = 0\n",
        "    total_f1 = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (reviews, labels) in enumerate(tqdm(data_loader)):\n",
        "      self.model.zero_grad()\n",
        "\n",
        "      reviews = reviews.to(self.device)\n",
        "      labels = labels.to(self.device)\n",
        "      output = self.model(reviews, labels=labels)\n",
        "      loss = output.loss\n",
        "\n",
        "      precision, recall, f1 = self.get_performance_metrics(output.logits.detach().cpu(), labels.cpu())\n",
        "      total_loss += loss\n",
        "      total_precision += precision\n",
        "      total_recall += recall\n",
        "      total_f1 += f1\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    precision = total_precision/len(data_loader)\n",
        "    recall = total_recall/len(data_loader)\n",
        "    f1 = total_f1/len(data_loader)\n",
        "    loss = total_loss/len(data_loader)\n",
        "\n",
        "    return precision, recall, f1, loss\n",
        "\n",
        "  def eval(self, data_loader):\n",
        "    self.model.eval()\n",
        "    total_recall = 0\n",
        "    total_precision = 0\n",
        "    total_f1 = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for (reviews, labels) in tqdm(data_loader):\n",
        "\n",
        "        reviews = reviews.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "        output = self.model(reviews, labels=labels)\n",
        "        loss = output.loss\n",
        "\n",
        "        precision, recall, f1 = self.get_performance_metrics(output.logits.detach().cpu(), labels.cpu())\n",
        "        total_loss += loss\n",
        "        total_precision += precision\n",
        "        total_recall += recall\n",
        "        total_f1 += f1\n",
        "\n",
        "    precision = total_precision/len(data_loader)\n",
        "    recall = total_recall/len(data_loader)\n",
        "    f1 = total_f1/len(data_loader)\n",
        "    loss = total_loss/len(data_loader)\n",
        "\n",
        "    return precision, recall, f1, loss\n",
        "\n",
        "  def save_transformer(self):\n",
        "    self.model.save_pretrained(self.save_path)\n",
        "    self.tokenizer.save_pretrained(self.save_path)\n",
        "\n",
        "  def execute(self):\n",
        "    last_best = 0\n",
        "    train_dataset = DatasetLoader(self.train_data, self.tokenizer)\n",
        "    train_data_loader = train_dataset.get_data_loaders(self.batch_size)\n",
        "    val_dataset = DatasetLoader(self.val_data, self.tokenizer)\n",
        "    val_data_loader = val_dataset.get_data_loaders(self.batch_size)\n",
        "    optimizer = torch.optim.AdamW(self.model.parameters(), lr = 3e-5, eps = 1e-8)\n",
        "    self.set_training_parameters()\n",
        "    for epoch_i in range(0, self.epochs):\n",
        "      train_precision, train_recall, train_f1, train_loss = self.train(train_data_loader, optimizer)\n",
        "      print(f'Epoch {epoch_i + 1}: train_loss: {train_loss:.4f} train_precision: {train_precision:.4f} train_recall: {train_recall:.4f} train_f1: {train_f1:.4f}')\n",
        "      val_precision, val_recall, val_f1, val_loss = self.eval(val_data_loader)\n",
        "      print(f'Epoch {epoch_i + 1}: val_loss: {val_loss:.4f} val_precision: {val_precision:.4f} val_recall: {val_recall:.4f} val_f1: {val_f1:.4f}')\n",
        "\n",
        "      if val_f1 > last_best:\n",
        "        print(\"Saving model..\")\n",
        "        self.save_transformer()\n",
        "        last_best = val_f1\n",
        "        print(\"Model saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMqn3uoTQkwV"
      },
      "source": [
        "#### **Training Experiment**\n",
        "\n",
        "Training our DistilBERT with all layers being trained.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yd1DUHuZa9Zz",
        "outputId": "c1f95ef1-7e3b-4bf6-9d70-5c4465399db2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing data..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3355/3355 [00:05<00:00, 663.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing data..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 578/578 [00:00<00:00, 756.26it/s]\n",
            "100%|██████████| 210/210 [02:45<00:00,  1.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train_loss: 0.9306 train_precision: 0.5261 train_recall: 0.5261 train_f1: 0.5261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 37/37 [00:09<00:00,  3.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: val_loss: 0.8349 val_precision: 0.5693 val_recall: 0.5693 val_f1: 0.5693\n",
            "Saving model..\n",
            "Model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 210/210 [02:45<00:00,  1.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: train_loss: 0.8518 train_precision: 0.5804 train_recall: 0.5804 train_f1: 0.5804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 37/37 [00:09<00:00,  3.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: val_loss: 0.8321 val_precision: 0.5946 val_recall: 0.5946 val_f1: 0.5946\n",
            "Saving model..\n",
            "Model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 210/210 [02:45<00:00,  1.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: train_loss: 0.7223 train_precision: 0.6484 train_recall: 0.6484 train_f1: 0.6484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 37/37 [00:09<00:00,  3.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: val_loss: 0.8789 val_precision: 0.5473 val_recall: 0.5473 val_f1: 0.5473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "options = {}\n",
        "options['batch_size'] = BATCH_SIZE\n",
        "options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "options['train_data'] = train_data\n",
        "options['val_data'] = val_data\n",
        "options['save_path'] = SAVE_PATH + '_all_training'\n",
        "options['epochs'] = EPOCHS\n",
        "options['training_type'] = 'all_training'\n",
        "trainer = Trainer(options)\n",
        "trainer.execute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JeEqQ1bQ4j3"
      },
      "source": [
        "## **Test Function**\n",
        "\n",
        "Here, we write the code for the testing of the models that we trained in the previous code blocks.\n",
        "\n",
        "The class below provides method to test a given model. It takes a dictionary with the following parameters:\n",
        "\n",
        "1.   device: The device to run the model on.\n",
        "2.   test_data: The test_data dataframe.\n",
        "3.   batch_size: The batch_size which is input to the model.\n",
        "4.   save_path: The directory of our saved model.\n",
        "\n",
        "We implement a single test step in the given loop inside the test() method. We pass the review and label in the given batch to the model, take the output and compute the Precision, Recall and F1 for that batch using the get_performance_metrics() method. We ensure that the loss is not propagated backwards.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URDu5ew3q5ke"
      },
      "outputs": [],
      "source": [
        "class Tester():\n",
        "\n",
        "  def __init__(self, options):\n",
        "    self.save_path = options['save_path']\n",
        "    self.device = options['device']\n",
        "    self.test_data = options['test_data']\n",
        "    self.batch_size = options['batch_size']\n",
        "    transformer = DistillBERT(self.save_path)\n",
        "    self.model, self.tokenizer = transformer.get_tokenizer_and_model()\n",
        "    self.model.to(self.device)\n",
        "\n",
        "  def get_performance_metrics(self, preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    precision = precision_score(labels_flat, pred_flat, zero_division=0, average='micro')\n",
        "    recall = recall_score(labels_flat, pred_flat, zero_division=0, average='micro')\n",
        "    f1 = f1_score(labels_flat, pred_flat, zero_division=0, average='micro')\n",
        "    return precision, recall, f1\n",
        "\n",
        "  def test(self, data_loader):\n",
        "    self.model.eval()\n",
        "    total_recall = 0\n",
        "    total_precision = 0\n",
        "    total_f1 = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for (reviews, labels) in tqdm(data_loader):\n",
        "\n",
        "        reviews = reviews.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "        output = self.model(reviews, labels=labels)\n",
        "        loss = output.loss\n",
        "\n",
        "        precision, recall, f1 = self.get_performance_metrics(output.logits.detach().cpu(), labels.cpu())\n",
        "        total_loss += loss\n",
        "        total_precision += precision\n",
        "        total_recall += recall\n",
        "        total_f1 += f1\n",
        "\n",
        "    precision = total_precision/len(data_loader)\n",
        "    recall = total_recall/len(data_loader)\n",
        "    f1 = total_f1/len(data_loader)\n",
        "    loss = total_loss/len(data_loader)\n",
        "\n",
        "    return precision, recall, f1, loss\n",
        "\n",
        "  def execute(self):\n",
        "    test_dataset = DatasetLoader(self.test_data, self.tokenizer)\n",
        "    test_data_loader = test_dataset.get_data_loaders(self.batch_size)\n",
        "\n",
        "    test_precision, test_recall, test_f1, test_loss = self.test(test_data_loader)\n",
        "\n",
        "    print()\n",
        "    print(f'test_loss: {test_loss:.4f} test_precision: {test_precision:.4f} test_recall: {test_recall:.4f} test_f1: {test_f1:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1d4bih3SMTM"
      },
      "source": [
        "**Notes: Now we run these blocks only after Training Experiment is completed and the best model is saved in the \"models\" folder.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79g8GwS4R3eB"
      },
      "source": [
        "#### **Testing Experiment**\n",
        "\n",
        "Testing our DistilBERT trained with all layers trainable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0wt64hdcyqR",
        "outputId": "fe2811e3-05ea-47c2-f41e-5fde903b1374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing data..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 579/579 [00:01<00:00, 563.57it/s]\n",
            "100%|██████████| 37/37 [00:10<00:00,  3.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "test_loss: 0.8685 test_precision: 0.5895 test_recall: 0.5895 test_f1: 0.5895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "options = {}\n",
        "options['batch_size'] = BATCH_SIZE\n",
        "options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "options['test_data'] = test_data\n",
        "options['save_path'] = SAVE_PATH + '_all_training'\n",
        "tester = Tester(options)\n",
        "tester.execute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEPJSuFSz17f"
      },
      "source": [
        "## **Results**\n",
        "\n",
        "Analysis of our models' performance:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlhzgwKx0c1r"
      },
      "source": [
        "Training Experiment:\n",
        "\n",
        "train_loss: 0.8518 train_precision: 0.5804 train_recall: 0.5804 train_f1: 0.5804\n",
        "\n",
        "val_loss: 0.8321 val_precision: 0.5946 val_recall: 0.5946 val_f1: 0.5946\n",
        "\n",
        "\n",
        "Testing Experiment:\n",
        "\n",
        "test_loss: 0.8685 test_precision: 0.5895 test_recall: 0.5895 test_f1: 0.5895\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}